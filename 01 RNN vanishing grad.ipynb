{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradient:\n",
    "RNNs: as more layers are added to the network with same activation funtion the gradient of the loss becomes smaller and smaller. This is because the gradient of the loss (chain rule) are multipied by small values. This means that the initial layers (the weight and biases associate with these layers are not getting updated) of the network are not getting updated through backpropagation. This is called the vanishing gradient problem which makes the network hard to train!\n",
    "\n",
    "### Solution:\n",
    "- Use ReLU activation function\n",
    "\n",
    "\n",
    "- Residual connections (skip connections): add the input to the output of the layer. This way the gradient of the loss is not multiplied by small values and the initial layers of the network are getting updated.\n",
    "\n",
    "\n",
    "## Links:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
